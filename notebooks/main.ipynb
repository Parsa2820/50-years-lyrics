{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parsa2820/50-years-lyrics/blob/master/notebooks/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxzVzDeBuhnQ"
      },
      "source": [
        "### Sharif University of Technology\n",
        "Department of Computer Engineering\n",
        "\n",
        "---\n",
        "# Modern Information Retrieval Course\n",
        "# **50 Years of the Best-Selling Music Artists Lyrics Comparison**\n",
        "### Homework 2\n",
        "### Dr. Asgari\n",
        "### Parsa Mohammadian — 98102284\n",
        "Spring 2022\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJHGT4kxTiK"
      },
      "source": [
        "## Introduction\n",
        "The art of music play an important role in the human world. Besides the instrumental aspect, lyrics and content of a music are also important. In this project, I will explore and compare the lyrics of the best-selling music artists in a 50 years period (from 1969 to 2019). This list is taken from [Visual Capitalist](https://www.visualcapitalist.com/chart-toppers-50-years-of-the-best-selling-music-artists/) website. They have also visualized this data in an awesome [video](https://www.youtube.com/watch?v=a3w8I8boc_I). For the reference, I will use the image bellow to pick artist that has been top-selling for at least one consecutive year. Since the dataset is not provided, I have hardcoded the artists and their info in the code.\n",
        "\n",
        "> Because of the dataset size limit of this project, I have only used some of the artists with introduced conditions. So I commented out the other artist in the code. The 4 remaining artist datasets size is 4.89 MB.\n",
        "\n",
        "![top-seller-chart](../resources/top-sellers-chart.jpg)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Artist:\n",
        "    def __init__(self, name: str, top_seller_begin_year: int, color: str):\n",
        "        self.name: str = name\n",
        "        self.top_seller_begin_year: int = top_seller_begin_year\n",
        "        self.color: str = color\n",
        "        self.lyrics: pd.DataFrame = None\n",
        "        self.normalized_words_count: int = 0\n",
        "        self.profanity_count: int = 0\n",
        "        self.vocab: set = set()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.name} ({self.top_seller_begin_year})'\n",
        "\n",
        "\n",
        "artists = [\n",
        "    Artist(\"The Beatles\", 1969, \"red\"),\n",
        "    # Artist(\"Elvis Presley\", 1973, \"blue\"),\n",
        "    Artist(\"Elton John\", 1975, \"green\"),\n",
        "    # Artist(\"Eagles\", 1977, \"yellow\"),\n",
        "    Artist(\"Michael Jackson\", 1980, \"orange\"),\n",
        "    # Artist(\"Madonna\", 1985, \"purple\"),\n",
        "    Artist(\"Eminem\", 2001, \"pink\"),\n",
        "    # Artist(\"Rihanna\", 2008, \"brown\"),\n",
        "    # Artist(\"Drake\", 2013, \"black\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hT2I1qByZFg"
      },
      "source": [
        "## Required Libraries\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU5Yl04FyggQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run this cell to install required python packages.\n",
        "Skip if you have already installed following packages.\n",
        "\"\"\"\n",
        "%pip install better-profanity\n",
        "%pip install matplotlib\n",
        "%pip install nltk\n",
        "%pip install pandas\n",
        "%pip install tqdm\n",
        "%pip install sentence_transformers\n",
        "%pip install sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkL_iCaOyv6b"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import string\n",
        "\n",
        "import better_profanity as bp\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hySArqrxjvR"
      },
      "source": [
        "## Dataset\n",
        "The dataset consists of multiple csv files, each file named as \"`<artist> Lyrics.csv`\" and contains all songs of the artist with their lyrics. I have written the script in [dataset/lyrics-script](../datasets/lyrics-script/genius.py) to generate the dataset. In order to get lyrics, I used [Genius](https://genius.com/) API. It worths mentioning that the order of the songs in every file is according to the number of views of the song in Genius website.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks4lM_1v0EOw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load data\n",
        "\"\"\"\n",
        "DATA_FILE_PREFIX = \"../datasets/\"\n",
        "DATA_FILE_SUFFIX = \" Lyrics.csv\"\n",
        "\n",
        "for artist in tqdm.tqdm(artists):\n",
        "    artist.lyrics = pd.read_csv(f\"{DATA_FILE_PREFIX}{artist.name}{DATA_FILE_SUFFIX}\")\n",
        "    artist.lyrics.rename(columns={\"Unnamed: 0\": \"idx\"}, inplace=True)\n",
        "    artist.lyrics.set_index(\"idx\", inplace=True)\n",
        "    artist.lyrics.dropna(inplace=True)\n",
        "\n",
        "for artist in artists:\n",
        "    print(f\"\\n{artist.name} with {artist.lyrics.size} songs\")\n",
        "    print(artist.lyrics.head(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for artist in artists:\n",
        "    artist.lyrics[\"song_lyrics_tokenized\"] = artist.lyrics[\"song_lyrics\"].apply(lambda x: nltk.word_tokenize(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_lower(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Converts the tokens to lower case.\n",
        "    \"\"\"\n",
        "    return [token.lower() for token in tokens]\n",
        "\n",
        "\n",
        "def remove_lyrics_tags(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes the tags added by Genius from the lyrics. \n",
        "    For example, [Chorus], [Verse 1], ...\n",
        "    \"\"\"\n",
        "    new_tokens = []\n",
        "    tag = False\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] == '[':\n",
        "            tag = True\n",
        "        elif tokens[i] == ']':\n",
        "            tag = False\n",
        "        elif not tag:\n",
        "            new_tokens.append(tokens[i])\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "def remove_song_name(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes the song name from the tokens.\n",
        "    \"\"\"\n",
        "    keyword = \"lyrics\"\n",
        "    if keyword in tokens:\n",
        "        return tokens[tokens.index(keyword) + 1:]\n",
        "    return tokens[:]\n",
        "\n",
        "\n",
        "def contains_any_of(token: list, chars: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns true if the token contains any of the characters in the given list.\n",
        "    \"\"\"\n",
        "    return any(char in token for char in chars)\n",
        "\n",
        "\n",
        "def remove_punctuation(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes punctuation from the given tokens.\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if not contains_any_of(token, string.punctuation+\"’‘•\")]\n",
        "\n",
        "\n",
        "def remove_music_characters(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes ♪ characters from the given tokens.\n",
        "    \"\"\"\n",
        "    return [token for token in tokens if not '♪' in token]\n",
        "\n",
        "\n",
        "def remove_stop_words(tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes stop words from the given tokens.\n",
        "    \"\"\"\n",
        "    remove_stop_words.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    with open(\"../resources/lyrics-stop-words\", \"r\") as f:\n",
        "        for line in f:\n",
        "            remove_stop_words.stop_words.add(line.strip())\n",
        "    return [token for token in tokens if token not in remove_stop_words.stop_words]\n",
        "\n",
        "\n",
        "def normalize_lyrics(tokens):\n",
        "    \"\"\"\n",
        "    Normalizes the tokens of the lyrics.\n",
        "    \"\"\"\n",
        "    normalization_functions = [to_lower, remove_lyrics_tags, remove_music_characters,\n",
        "                               remove_song_name, remove_punctuation, remove_stop_words]\n",
        "    return functools.reduce(lambda x, f: f(x), normalization_functions, tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for artist in artists:\n",
        "    artist.lyrics[\"song_lyrics_normalized\"] = artist.lyrics[\"song_lyrics_tokenized\"].apply(normalize_lyrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Here I have used Porter2 stemming algorithm to stem the words in the lyrics. The reason for preferring stemming over lemmatization is that it is more suitable for lyrics. We can see tons of examples in the lyrics which used ing form of the verb like \"talkin\" instead of \"talking\". In such a situation, the stemmer will remove suffix and leave the root word but lemmatization will not.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "for artist in artists:\n",
        "    artist.lyrics[\"song_lyrics_stemmed\"] = (artist.lyrics[\"song_lyrics_normalized\"].apply(lambda x: [stemmer.stem(t) for t in x]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Frequency Analysis and Find Stop Words\n",
        "\n",
        "Although removing stop words was used in th normalization process, there are some extra worthless words in lyrics that we can determine by frequency analysis. So after the first frequency analysis, I gathered some other stop words in \"`resources/lyrics-stop-words`\" file and modified the normalization process to remove them.\n",
        "\n",
        "> Keep in mind that tokens are stemmed, so it is possible to see misspelled words in the frequency analysis result.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for artist in artists:\n",
        "    all_lyrics = itertools.chain(*artist.lyrics[\"song_lyrics_stemmed\"])\n",
        "    artist.freq_dist = nltk.FreqDist(all_lyrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "freq_dist_dic = {}\n",
        "\n",
        "for artist in artists:\n",
        "    freq_dist = artist.freq_dist.most_common(20)\n",
        "    freq_dist_profanity_filtered = []\n",
        "    for word, count in freq_dist:\n",
        "        freq_dist_profanity_filtered.append((bp.profanity.censor(word), count))\n",
        "    freq_dist_dic[artist.name] = freq_dist_profanity_filtered\n",
        "\n",
        "freq_dist_df = pd.DataFrame(freq_dist_dic)\n",
        "freq_dist_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profanity and Vocabulary  Analysis \n",
        "\n",
        "First I provide number (percent) of profanity words for each artist. Then I count each artists unique used words and by that I can see the vocabulary size of each artist.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for artist in artists:\n",
        "    def count(tokens):\n",
        "        artist.normalized_words_count += len(tokens)\n",
        "        artist.vocab.update(tokens)\n",
        "        for token in tokens:\n",
        "            if bp.profanity.contains_profanity(token):\n",
        "                artist.profanity_count += 1\n",
        "    artist.lyrics[\"song_lyrics_stemmed\"].apply(count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "plt.title(\"Vocabulary Size\")\n",
        "plt.xlabel(\"Artists\")\n",
        "plt.ylabel(\"Number of All Normalized Words\")\n",
        "plt.bar(range(len(artists)), [artist.normalized_words_count for artist in artists], color=[artist.color for artist in artists])\n",
        "plt.xticks(range(len(artists)), labels=[f\"{artist}\" for artist in artists])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "plt.title(\"Vocabulary Diversity\")\n",
        "plt.xlabel(\"Artists\")\n",
        "plt.ylabel(\"Number of Unique Normalized Words\")\n",
        "plt.bar(range(len(artists)), [len(artist.vocab) for artist in artists], color=[artist.color for artist in artists])\n",
        "plt.xticks(range(len(artists)), labels=[f\"{artist}\" for artist in artists])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 10))\n",
        "plt.title(\"Profanity Count\")\n",
        "plt.xlabel(\"Artists\")\n",
        "plt.ylabel(\"Count of Profanity Words\")\n",
        "plt.bar(range(len(artists)), [artist.profanity_count for artist in artists], color=[artist.color for artist in artists])\n",
        "plt.xticks(range(len(artists)), labels=[f\"{artist}\" for artist in artists])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "percent = lambda artist: artist.profanity_count / artist.normalized_words_count * 100\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.title(\"Profanity Percentage\")\n",
        "plt.xlabel(\"Artists\")\n",
        "plt.ylabel(\"Percent of Profanity Words\")\n",
        "plt.bar(range(len(artists)), [percent(artist) for artist in artists], color=[artist.color for artist in artists])\n",
        "plt.xticks(range(len(artists)), labels=[f\"{artist}\" for artist in artists])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see in four above figures, the newer artists are using more diverse vocabulary. They also tend to use more profanity words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Lyrics!\n",
        "\n",
        "This is the last part of the project. In this section, I want to vectorize the lyrics of each song to a vector of a fixed size which is 384. Then I merge all the lyrics into a matrix of size `(number of songs, vector size)`. After that I use a dimension reduction technique to reduce the dimension of the matrix to `(number of songs, 2)` which is the most suitable for visualization.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "features = [f\"feature_{i}\" for i in range(384)]\n",
        "song_vector_df = pd.DataFrame(columns=[\"artist\"] + features)\n",
        "\n",
        "for artist in artists:\n",
        "    for song in tqdm.tqdm(artist.lyrics[\"song_lyrics_stemmed\"], desc=f\"{artist.name}\"):\n",
        "        song_str = \" \".join(song)\n",
        "        song_ds = pd.Series([artist.name] + list(model.encode(song_str)), index=song_vector_df.columns)\n",
        "        song_vector_df = song_vector_df.append(song_ds, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = StandardScaler().fit_transform(song_vector_df.loc[:, features])\n",
        "y = song_vector_df.loc[:, [\"artist\"]]\n",
        "pca = PCA(n_components=2)\n",
        "pcs = pca.fit_transform(x)\n",
        "pcs_df = pd.DataFrame(data = pcs, columns = ['pc1', 'pc2'])\n",
        "song_two_vector_df = pd.concat([pcs_df, y], axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "plt.xlabel(\"Principal Component 1\", fontsize=15)\n",
        "plt.ylabel(\"Principal Component 2\", fontsize=15)\n",
        "for artist in artists:\n",
        "    songs = song_two_vector_df['artist'] == artist.name\n",
        "    pc1 = song_two_vector_df[songs]['pc1']\n",
        "    pc2 = song_two_vector_df[songs]['pc2']\n",
        "    plt.scatter(pc1, pc2, c=artist.color)\n",
        "plt.legend([artist.name for artist in artists]);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "every_two_artists = itertools.combinations(artists, 2)\n",
        "\n",
        "fig, ax = plt.subplots(len(every_two_artists), 1, figsize=(20, 10*len(every_two_artists)))\n",
        "\n",
        "for idx, artist1, artist2 in enumerate(every_two_artists):\n",
        "    songs1 = song_two_vector_df['artist'] == artist1.name\n",
        "    songs2 = song_two_vector_df['artist'] == artist2.name\n",
        "    pc1 = song_two_vector_df[songs1]['pc1']\n",
        "    pc2 = song_two_vector_df[songs1]['pc2']\n",
        "    ax[idx].scatter(pc1, pc2, c=artist1.color)\n",
        "    pc1 = song_two_vector_df[songs2]['pc1']\n",
        "    pc2 = song_two_vector_df[songs2]['pc2']\n",
        "    ax[idx].scatter(pc1, pc2, c=artist2.color)\n",
        "    ax[idx].legend([artist1.name, artist2.name])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPOgAEDMIDhOp1gPYF4emtB",
      "collapsed_sections": [
        "8hySArqrxjvR"
      ],
      "include_colab_link": true,
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
